"""
Universal Engine - Production Grade v2.0
ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¡°ìœ¨ ë° ë¸Œëœë“œë³„ ì²˜ë¦¬

Features:
    - ë¸Œëœë“œë³„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
    - ë©€í‹° ìŠ¤í…Œì´ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
    - í—¬ìŠ¤ ì²´í¬ ë° ìƒíƒœ ê´€ë¦¬
    - Graceful ì—ëŸ¬ í•¸ë“¤ë§
    - ìš”ì²­ ì œí•œ (Rate Limiting)
    - ë¯¸ë“¤ì›¨ì–´ ì§€ì›
"""

from typing import Dict, List, Optional, Generator, Any, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import deque
import threading
import asyncio
import time
import logging
import json
import hashlib

from app.core.context import QueryContext, QuestionType, ProcessingStage
from app.core.routing import QuestionRouter
from app.core.pipeline import Pipeline
from app.services.platform.config_manager import ConfigManager
from app.features.registry import FeatureRegistry
from app.services.platform.analytics import get_analytics_service
from app.models.chat import ChatRequest, ChatResponse, RetrievalContext
from app.services.shared.cache import get_cache_client

# Production Grade v2.0 Filters
from app.filters import (
    ValidationFilter,
    ValidationConfig,
    ValidationResult,
    ValidationStatus,
    OverallGrade,
    FilterType,
    QualityConfig,
    TrustConfig,
    RelevanceConfig,
)

logger = logging.getLogger(__name__)


# ============================================================
# Feature Handler Manager
# ============================================================

class FeatureHandlerManager:
    """
    Feature Handler ê´€ë¦¬ì
    FeatureRegistryë¥¼ ë˜í•‘í•˜ì—¬ engineì—ì„œ ì‚¬ìš©
    """

    def __init__(self, brand_id: str, brand_config: Dict[str, Any]):
        self.brand_id = brand_id
        self.brand_config = brand_config
        self._handler_cache: Dict[str, Any] = {}

    def get_handler(self, feature_name: str):
        """
        Feature handler ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

        Args:
            feature_name: ê¸°ëŠ¥ ì´ë¦„ (question_type)

        Returns:
            FeatureHandler ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        # ìºì‹œì—ì„œ í™•ì¸
        if feature_name in self._handler_cache:
            return self._handler_cache[feature_name]

        # FeatureRegistryì—ì„œ í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
        handler = FeatureRegistry.get_handler(feature_name, self.brand_config)

        if handler:
            self._handler_cache[feature_name] = handler

        return handler

    def has_handler(self, feature_name: str) -> bool:
        """Feature handler ì¡´ì¬ ì—¬ë¶€"""
        return FeatureRegistry.has_feature(feature_name)

    def list_features(self) -> List[str]:
        """ë“±ë¡ëœ feature ëª©ë¡"""
        return FeatureRegistry.list_features()


# ============================================================
# Enums
# ============================================================

class EngineState(str, Enum):
    """ì—”ì§„ ìƒíƒœ"""
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    DEGRADED = "degraded"
    SHUTTING_DOWN = "shutting_down"
    STOPPED = "stopped"


class ErrorType(str, Enum):
    """ì—ëŸ¬ ìœ í˜•"""
    VALIDATION = "validation"
    ROUTING = "routing"
    RETRIEVAL = "retrieval"
    GENERATION = "generation"
    CACHE = "cache"
    RATE_LIMIT = "rate_limit"
    TIMEOUT = "timeout"
    INTERNAL = "internal"


# ============================================================
# Configuration
# ============================================================

@dataclass
class EngineConfig:
    """ì—”ì§„ ì„¤ì •"""
    # ìºì‹œ ì„¤ì •
    cache_enabled: bool = True
    cache_ttl: int = 3600  # 1ì‹œê°„

    # Rate Limiting
    rate_limit_enabled: bool = True
    rate_limit_requests: int = 100  # ë¶„ë‹¹ ìš”ì²­ ìˆ˜
    rate_limit_window: int = 60  # ì´ˆ

    # íƒ€ì„ì•„ì›ƒ
    request_timeout: float = 30.0  # ì´ˆ
    retrieval_timeout: float = 10.0
    generation_timeout: float = 20.0

    # ì¬ì‹œë„
    max_retries: int = 2
    retry_delay: float = 0.5

    # ìŠ¤íŠ¸ë¦¬ë°
    stream_chunk_size: int = 100

    # ë””ë²„ê·¸
    debug_mode: bool = False

    # === Validation Filter v2.0 ì„¤ì • ===
    validation_enabled: bool = True
    validation_min_grade: str = "D"  # ìµœì†Œ í—ˆìš© ë“±ê¸‰ (A, B, C, D, F)
    validation_auto_retry: bool = True  # ë‚®ì€ ë“±ê¸‰ì‹œ ìë™ ì¬ìƒì„±
    validation_max_retries: int = 2  # ì¬ìƒì„± ìµœëŒ€ ì‹œë„ íšŸìˆ˜
    validation_include_suggestions: bool = True  # ì‘ë‹µì— ê°œì„  ì œì•ˆ í¬í•¨

    @classmethod
    def from_dict(cls, data: Dict) -> 'EngineConfig':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ìƒì„±"""
        return cls(
            cache_enabled=data.get('cache_enabled', True),
            cache_ttl=data.get('cache_ttl', 3600),
            rate_limit_enabled=data.get('rate_limit_enabled', True),
            rate_limit_requests=data.get('rate_limit_requests', 100),
            rate_limit_window=data.get('rate_limit_window', 60),
            request_timeout=data.get('request_timeout', 30.0),
            retrieval_timeout=data.get('retrieval_timeout', 10.0),
            generation_timeout=data.get('generation_timeout', 20.0),
            max_retries=data.get('max_retries', 2),
            retry_delay=data.get('retry_delay', 0.5),
            debug_mode=data.get('debug_mode', False),
            # Validation v2.0
            validation_enabled=data.get('validation_enabled', True),
            validation_min_grade=data.get('validation_min_grade', 'D'),
            validation_auto_retry=data.get('validation_auto_retry', True),
            validation_max_retries=data.get('validation_max_retries', 2),
            validation_include_suggestions=data.get('validation_include_suggestions', True),
        )


# ============================================================
# Metrics
# ============================================================

@dataclass
class RequestMetrics:
    """ìš”ì²­ë³„ ë©”íŠ¸ë¦­"""
    request_id: str
    start_time: float
    end_time: Optional[float] = None
    routing_time: float = 0.0
    retrieval_time: float = 0.0
    generation_time: float = 0.0
    validation_time: float = 0.0
    cache_hit: bool = False
    error: Optional[str] = None
    question_type: Optional[str] = None
    retrieval_count: int = 0
    # Validation v2.0 ë©”íŠ¸ë¦­
    validation_score: Optional[float] = None
    validation_grade: Optional[str] = None
    validation_retries: int = 0

    @property
    def total_time(self) -> float:
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time

    def to_dict(self) -> Dict[str, Any]:
        return {
            'request_id': self.request_id,
            'total_time_ms': round(self.total_time * 1000, 2),
            'routing_time_ms': round(self.routing_time * 1000, 2),
            'retrieval_time_ms': round(self.retrieval_time * 1000, 2),
            'generation_time_ms': round(self.generation_time * 1000, 2),
            'validation_time_ms': round(self.validation_time * 1000, 2),
            'cache_hit': self.cache_hit,
            'question_type': self.question_type,
            'retrieval_count': self.retrieval_count,
            'validation_score': self.validation_score,
            'validation_grade': self.validation_grade,
            'validation_retries': self.validation_retries,
            'error': self.error,
        }


class EngineMetrics:
    """ì—”ì§„ ì „ì²´ ë©”íŠ¸ë¦­"""

    def __init__(self, max_history: int = 1000):
        self.max_history = max_history
        self._requests: deque = deque(maxlen=max_history)
        self._lock = threading.Lock()

        # ì¹´ìš´í„°
        self.total_requests: int = 0
        self.successful_requests: int = 0
        self.failed_requests: int = 0
        self.cache_hits: int = 0
        self.rate_limited_requests: int = 0

        # ì‹œì‘ ì‹œê°„
        self.start_time: float = time.time()

    def record_request(self, metrics: RequestMetrics):
        """ìš”ì²­ ê¸°ë¡"""
        with self._lock:
            self._requests.append(metrics)
            self.total_requests += 1

            if metrics.error:
                self.failed_requests += 1
            else:
                self.successful_requests += 1

            if metrics.cache_hit:
                self.cache_hits += 1

    def record_rate_limit(self):
        """Rate limit ê¸°ë¡"""
        with self._lock:
            self.rate_limited_requests += 1

    def get_average_response_time(self, last_n: int = 100) -> float:
        """í‰ê·  ì‘ë‹µ ì‹œê°„ (ms)"""
        with self._lock:
            recent = list(self._requests)[-last_n:]
            if not recent:
                return 0.0
            times = [r.total_time for r in recent if r.end_time]
            return round(sum(times) / len(times) * 1000, 2) if times else 0.0

    def get_error_rate(self, last_n: int = 100) -> float:
        """ì—ëŸ¬ìœ¨ (%)"""
        with self._lock:
            recent = list(self._requests)[-last_n:]
            if not recent:
                return 0.0
            errors = sum(1 for r in recent if r.error)
            return round(errors / len(recent) * 100, 2)

    def get_cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ (%)"""
        if self.total_requests == 0:
            return 0.0
        return round(self.cache_hits / self.total_requests * 100, 2)

    def get_requests_per_minute(self) -> float:
        """ë¶„ë‹¹ ìš”ì²­ ìˆ˜"""
        elapsed_minutes = (time.time() - self.start_time) / 60
        if elapsed_minutes < 0.1:
            return 0.0
        return round(self.total_requests / elapsed_minutes, 2)

    def get_summary(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ìš”ì•½"""
        return {
            'uptime_seconds': round(time.time() - self.start_time, 2),
            'total_requests': self.total_requests,
            'successful_requests': self.successful_requests,
            'failed_requests': self.failed_requests,
            'cache_hits': self.cache_hits,
            'cache_hit_rate': self.get_cache_hit_rate(),
            'rate_limited_requests': self.rate_limited_requests,
            'avg_response_time_ms': self.get_average_response_time(),
            'error_rate': self.get_error_rate(),
            'requests_per_minute': self.get_requests_per_minute(),
        }


# ============================================================
# Rate Limiter
# ============================================================

class RateLimiter:
    """Rate Limiter (Token Bucket)"""

    def __init__(self, max_requests: int, window_seconds: int):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self._requests: deque = deque()
        self._lock = threading.Lock()

    def is_allowed(self) -> bool:
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€"""
        with self._lock:
            now = time.time()
            window_start = now - self.window_seconds

            # ìœˆë„ìš° ë°–ì˜ ìš”ì²­ ì œê±°
            while self._requests and self._requests[0] < window_start:
                self._requests.popleft()

            # ì œí•œ í™•ì¸
            if len(self._requests) >= self.max_requests:
                return False

            # í˜„ì¬ ìš”ì²­ ê¸°ë¡
            self._requests.append(now)
            return True

    def get_remaining(self) -> int:
        """ë‚¨ì€ ìš”ì²­ ìˆ˜"""
        with self._lock:
            now = time.time()
            window_start = now - self.window_seconds

            # ìœˆë„ìš° ë°–ì˜ ìš”ì²­ ì œê±°
            while self._requests and self._requests[0] < window_start:
                self._requests.popleft()

            return max(0, self.max_requests - len(self._requests))

    def reset(self):
        """ë¦¬ì…‹"""
        with self._lock:
            self._requests.clear()


# ============================================================
# Engine Error
# ============================================================

class EngineError(Exception):
    """ì—”ì§„ ì—ëŸ¬"""

    def __init__(
        self,
        message: str,
        error_type: ErrorType = ErrorType.INTERNAL,
        details: Dict = None
    ):
        super().__init__(message)
        self.message = message
        self.error_type = error_type
        self.details = details or {}
        self.timestamp = datetime.now()

    def to_dict(self) -> Dict[str, Any]:
        return {
            'message': self.message,
            'type': self.error_type.value,
            'details': self.details,
            'timestamp': self.timestamp.isoformat(),
        }


# ============================================================
# Middleware
# ============================================================

class Middleware:
    """ë¯¸ë“¤ì›¨ì–´ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    def before_request(self, context: QueryContext) -> QueryContext:
        """ìš”ì²­ ì „ì²˜ë¦¬"""
        return context

    def after_response(self, context: QueryContext, response: ChatResponse) -> ChatResponse:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        return response

    def on_error(self, context: QueryContext, error: Exception) -> Optional[ChatResponse]:
        """ì—ëŸ¬ ì²˜ë¦¬ (None ë°˜í™˜ì‹œ ì—ëŸ¬ ì „íŒŒ)"""
        return None


class LoggingMiddleware(Middleware):
    """ë¡œê¹… ë¯¸ë“¤ì›¨ì–´"""

    def before_request(self, context: QueryContext) -> QueryContext:
        logger.info(f"[Request] brand={context.brand_id}, question={context.question[:50]}...")
        return context

    def after_response(self, context: QueryContext, response: ChatResponse) -> ChatResponse:
        logger.info(f"[Response] brand={context.brand_id}, status=success")
        return response

    def on_error(self, context: QueryContext, error: Exception) -> Optional[ChatResponse]:
        logger.error(f"[Error] brand={context.brand_id}, error={str(error)}")
        return None


class ValidationMiddleware(Middleware):
    """ê²€ì¦ ë¯¸ë“¤ì›¨ì–´"""

    MAX_QUESTION_LENGTH = 2000
    MIN_QUESTION_LENGTH = 2

    def before_request(self, context: QueryContext) -> QueryContext:
        # ì§ˆë¬¸ ê¸¸ì´ ê²€ì¦
        if len(context.question) < self.MIN_QUESTION_LENGTH:
            raise EngineError(
                "ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.",
                ErrorType.VALIDATION,
                {'min_length': self.MIN_QUESTION_LENGTH}
            )

        if len(context.question) > self.MAX_QUESTION_LENGTH:
            raise EngineError(
                "ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.",
                ErrorType.VALIDATION,
                {'max_length': self.MAX_QUESTION_LENGTH}
            )

        return context


# ============================================================
# Universal Engine
# ============================================================

class UniversalEngine:
    """
    Universal Engine - Production Grade

    ëª¨ë“  ë¸Œëœë“œì— ëŒ€í•œ í†µí•© ì²˜ë¦¬ ì—”ì§„

    Features:
        - ë¸Œëœë“œë³„ ìë™ ì„¤ì • ë¡œë“œ
        - ë©€í‹° ìŠ¤í…Œì´ì§€ ì§ˆë¬¸ ë¼ìš°íŒ…
        - RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
        - ìºì‹± ë° Rate Limiting
        - ë¯¸ë“¤ì›¨ì–´ ì²´ì¸
        - í—¬ìŠ¤ ì²´í¬
    """

    # ë¸Œëœë“œë³„ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìºì‹œ
    _instances: Dict[str, 'UniversalEngine'] = {}
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls, brand_id: str) -> 'UniversalEngine':
        """
        ë¸Œëœë“œë³„ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Thread-safe ì‹±ê¸€í†¤)

        Args:
            brand_id: ë¸Œëœë“œ ID

        Returns:
            ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
        """
        with cls._lock:
            if brand_id not in cls._instances:
                cls._instances[brand_id] = cls(brand_id)
            return cls._instances[brand_id]

    @classmethod
    def clear_cache(cls, brand_id: str = None):
        """
        ì—”ì§„ ìºì‹œ í´ë¦¬ì–´

        Args:
            brand_id: ë¸Œëœë“œ ID (Noneì´ë©´ ì „ì²´ í´ë¦¬ì–´)
        """
        with cls._lock:
            if brand_id:
                if brand_id in cls._instances:
                    engine = cls._instances[brand_id]
                    engine._shutdown()
                    del cls._instances[brand_id]
                    logger.info(f"Cleared engine cache for: {brand_id}")
            else:
                for engine in cls._instances.values():
                    engine._shutdown()
                cls._instances.clear()
                logger.info("Cleared all engine caches")

    @classmethod
    def get_all_instances(cls) -> Dict[str, 'UniversalEngine']:
        """ëª¨ë“  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        with cls._lock:
            return dict(cls._instances)

    def __init__(self, brand_id: str):
        """
        Args:
            brand_id: ë¸Œëœë“œ ID
        """
        self._state = EngineState.INITIALIZING
        self.brand_id = brand_id

        # ì„¤ì • ë¡œë“œ
        self.brand_config = ConfigManager.load_brand_config(brand_id)
        self.engine_config = EngineConfig.from_dict(
            self.brand_config.get('engine', {})
        )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.router = QuestionRouter(self.brand_config)
        self.pipeline = Pipeline(self.brand_config)
        self.feature_manager = FeatureHandlerManager(
            brand_id=brand_id,
            brand_config=self.brand_config
        )

        # ì„œë¹„ìŠ¤
        self.cache = get_cache_client()
        self.analytics = get_analytics_service()

        # Rate Limiter
        self.rate_limiter = RateLimiter(
            max_requests=self.engine_config.rate_limit_requests,
            window_seconds=self.engine_config.rate_limit_window
        )

        # === Validation Filter v2.0 ì´ˆê¸°í™” ===
        self.validation_filter = ValidationFilter(
            config=ValidationConfig(
                trust_config=TrustConfig(),
                quality_config=QualityConfig(language="ko"),
                relevance_config=RelevanceConfig(language="ko"),
            )
        )
        # ë“±ê¸‰ ë§¤í•‘ (ë¬¸ìì—´ -> ì—´ê±°í˜•)
        self._grade_priority = {
            'A': 5, 'B': 4, 'C': 3, 'D': 2, 'F': 1
        }

        # ë©”íŠ¸ë¦­
        self.metrics = EngineMetrics()

        # ë¯¸ë“¤ì›¨ì–´
        self._middlewares: List[Middleware] = [
            LoggingMiddleware(),
            ValidationMiddleware(),
        ]

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self._state = EngineState.READY
        self._initialized_at = datetime.now()

        logger.info(f"Engine initialized for brand: {brand_id} (Validation v2.0 enabled)")

    # === Public API ===

    def ask(
        self,
        question: str,
        conversation_history: List[Dict] = None,
        use_cache: bool = True,
        request_id: str = None
    ) -> ChatResponse:
        """
        ì§ˆë¬¸ ì²˜ë¦¬ (ë™ê¸°)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            request_id: ìš”ì²­ ID (ì—†ìœ¼ë©´ ìë™ ìƒì„±)

        Returns:
            ChatResponse
        """
        # ìš”ì²­ ID ìƒì„±
        if not request_id:
            request_id = self._generate_request_id(question)

        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        request_metrics = RequestMetrics(
            request_id=request_id,
            start_time=time.time()
        )

        try:
            # ìƒíƒœ í™•ì¸
            self._check_engine_state()

            # Rate Limiting
            if self.engine_config.rate_limit_enabled:
                if not self.rate_limiter.is_allowed():
                    self.metrics.record_rate_limit()
                    raise EngineError(
                        "ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        ErrorType.RATE_LIMIT,
                        {'remaining': 0}
                    )

            # ìºì‹œ í™•ì¸
            use_cache = use_cache and self.engine_config.cache_enabled
            if use_cache:
                cached_response = self._get_cached_response(question)
                if cached_response:
                    request_metrics.cache_hit = True
                    request_metrics.end_time = time.time()
                    self.metrics.record_request(request_metrics)

                    self.analytics.track_event(
                        'cache_hit',
                        self.brand_id,
                        {'question': question[:100], 'request_id': request_id}
                    )
                    return cached_response

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = QueryContext(
                brand_id=self.brand_id,
                question=question,
                conversation_history=conversation_history or []
            )
            context.add_metadata('request_id', request_id)

            # ë¯¸ë“¤ì›¨ì–´ - before_request
            for middleware in self._middlewares:
                context = middleware.before_request(context)

            # 1. ì§ˆë¬¸ ë¼ìš°íŒ…
            routing_start = time.time()
            question_type = self.router.route(context)
            request_metrics.routing_time = time.time() - routing_start
            request_metrics.question_type = question_type

            # 2. Feature Handler í™•ì¸
            feature_handler = self.feature_manager.get_handler(question_type)

            # Feature handler ê²°ê³¼ ì €ì¥
            feature_result = None
            use_pipeline = True

            if feature_handler and feature_handler.can_handle(question, context.to_dict()):
                logger.info(f"Using feature handler: {question_type}")

                feature_result = feature_handler.process(question, context.to_dict())

                # Feature handlerê°€ ì‘ë‹µì„ ìƒì„±í•œ ê²½ìš°
                if feature_result.get('response'):
                    context.set_response(feature_result.get('response'))
                    context.add_metadata('handled_by', f'feature_{question_type}')
                    use_pipeline = False
                else:
                    # ì‘ë‹µì´ Noneì´ë©´ íŒŒì´í”„ë¼ì¸ì— ìœ„ì„
                    logger.info(f"Feature handler {question_type} delegated to pipeline")
                    context.add_metadata('feature_preprocessed', question_type)

            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (feature handlerê°€ ì—†ê±°ë‚˜ ì‘ë‹µì„ ìœ„ì„í•œ ê²½ìš°)
            if use_pipeline:
                # 3. ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                retrieval_start = time.time()
                context = self.pipeline._retrieve(context)
                request_metrics.retrieval_time = time.time() - retrieval_start
                request_metrics.retrieval_count = context.get_total_retrieval_count()

                generation_start = time.time()
                context = self.pipeline._generate(context)
                request_metrics.generation_time = time.time() - generation_start

                context.add_metadata('handled_by', 'pipeline')

            # Feature handlerì˜ ë©”íƒ€ë°ì´í„°/í•„í„° ê²°ê³¼ ë³‘í•©
            if feature_result:
                if feature_result.get('metadata'):
                    for key, value in feature_result['metadata'].items():
                        context.add_metadata(key, value)
                if feature_result.get('filter_results'):
                    context.add_metadata('filter_results', feature_result['filter_results'])

            # === 4. Validation Filter v2.0 ì ìš© ===
            if self.engine_config.validation_enabled and context.response:
                validation_start = time.time()
                context, request_metrics = self._validate_and_retry(
                    context, request_metrics, question
                )
                request_metrics.validation_time = time.time() - validation_start

            # ì‘ë‹µ ìƒì„±
            response = self._build_response(context, request_metrics)

            # ë¯¸ë“¤ì›¨ì–´ - after_response
            for middleware in reversed(self._middlewares):
                response = middleware.after_response(context, response)

            # ìºì‹±
            if use_cache and context.response:
                self._cache_response(question, response)

            # ë©”íŠ¸ë¦­ ê¸°ë¡
            request_metrics.end_time = time.time()
            self.metrics.record_request(request_metrics)

            # ë¶„ì„ ì¶”ì 
            self.analytics.track_event(
                'query',
                self.brand_id,
                {
                    'question': question[:100],
                    'question_type': question_type,
                    'request_id': request_id,
                    **request_metrics.to_dict()
                }
            )

            return response

        except EngineError as e:
            return self._handle_error(e, request_metrics, question)

        except Exception as e:
            engine_error = EngineError(str(e), ErrorType.INTERNAL)
            return self._handle_error(engine_error, request_metrics, question)

    def ask_stream(
        self,
        question: str,
        conversation_history: List[Dict] = None
    ) -> Generator[str, None, None]:
        """
        ì§ˆë¬¸ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë°)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬

        Yields:
            ì‘ë‹µ ì²­í¬
        """
        try:
            # ìƒíƒœ í™•ì¸
            self._check_engine_state()

            # Rate Limiting
            if self.engine_config.rate_limit_enabled:
                if not self.rate_limiter.is_allowed():
                    yield "Error: Rate limit exceeded"
                    return

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = QueryContext(
                brand_id=self.brand_id,
                question=question,
                conversation_history=conversation_history or []
            )

            # ë¼ìš°íŒ…
            question_type = self.router.route(context)

            # Retrieval
            context = self.pipeline._retrieve(context)

            # ìŠ¤íŠ¸ë¦¬ë° Generation
            from app.services.shared.llm import get_llm_client
            llm = get_llm_client()

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            generation_config = self.brand_config.get('generation', {}).get(question_type, {})
            prompt_path = generation_config.get('prompt', 'conversational/default.txt')

            system_prompt = self.pipeline._load_prompt(prompt_path) or "You are a helpful assistant."

            # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            context_str = self._format_retrieval_context(context)
            user_prompt = f"""Question: {question}

Context:
{context_str}

Answer the question based on the context above."""

            # ìŠ¤íŠ¸ë¦¬ë°
            for chunk in llm.stream(
                prompt=user_prompt,
                system_prompt=system_prompt,
                model_variant="full"
            ):
                yield chunk

        except EngineError as e:
            yield f"Error: {e.message}"

        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield f"Error: {str(e)}"

    async def ask_async(
        self,
        question: str,
        conversation_history: List[Dict] = None,
        use_cache: bool = True
    ) -> ChatResponse:
        """
        ì§ˆë¬¸ ì²˜ë¦¬ (ë¹„ë™ê¸°)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            ChatResponse
        """
        # ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self.ask(question, conversation_history, use_cache)
        )

    # === Health & Status ===

    def health_check(self) -> Dict[str, Any]:
        """
        í—¬ìŠ¤ ì²´í¬

        Returns:
            í—¬ìŠ¤ ìƒíƒœ ì •ë³´
        """
        health = {
            'status': 'healthy',
            'brand_id': self.brand_id,
            'engine_state': self._state.value,
            'initialized_at': self._initialized_at.isoformat(),
            'uptime_seconds': round(time.time() - self.metrics.start_time, 2),
            'checks': {}
        }

        # ê° ì»´í¬ë„ŒíŠ¸ ì²´í¬
        try:
            # Router ì²´í¬
            health['checks']['router'] = 'ok' if self.router else 'missing'

            # Pipeline ì²´í¬
            health['checks']['pipeline'] = 'ok' if self.pipeline else 'missing'

            # Cache ì²´í¬
            try:
                self.cache.ping() if hasattr(self.cache, 'ping') else None
                health['checks']['cache'] = 'ok'
            except Exception:
                health['checks']['cache'] = 'degraded'

            # ë©”íŠ¸ë¦­ ìš”ì•½
            health['metrics'] = self.metrics.get_summary()

            # ì—ëŸ¬ìœ¨ í™•ì¸
            error_rate = self.metrics.get_error_rate()
            if error_rate > 50:
                health['status'] = 'unhealthy'
            elif error_rate > 20:
                health['status'] = 'degraded'

        except Exception as e:
            health['status'] = 'unhealthy'
            health['error'] = str(e)

        return health

    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        return {
            'brand_id': self.brand_id,
            'state': self._state.value,
            'initialized_at': self._initialized_at.isoformat(),
            'rate_limit_remaining': self.rate_limiter.get_remaining(),
            'metrics': self.metrics.get_summary(),
        }

    # === Middleware Management ===

    def add_middleware(self, middleware: Middleware):
        """ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€"""
        self._middlewares.append(middleware)
        logger.info(f"Added middleware: {middleware.__class__.__name__}")

    def remove_middleware(self, middleware_class: type):
        """ë¯¸ë“¤ì›¨ì–´ ì œê±°"""
        self._middlewares = [
            m for m in self._middlewares
            if not isinstance(m, middleware_class)
        ]

    # === Configuration ===

    def reload_config(self):
        """ì„¤ì • ë¦¬ë¡œë“œ"""
        logger.info(f"Reloading config for: {self.brand_id}")

        # ì„¤ì • ë¦¬ë¡œë“œ
        self.brand_config = ConfigManager.reload_config(self.brand_id)
        self.engine_config = EngineConfig.from_dict(
            self.brand_config.get('engine', {})
        )

        # ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”
        self.router = QuestionRouter(self.brand_config)
        self.pipeline = Pipeline(self.brand_config)
        self.feature_manager = FeatureHandlerManager(
            brand_id=self.brand_id,
            brand_config=self.brand_config
        )

        # Rate Limiter ì—…ë°ì´íŠ¸
        self.rate_limiter = RateLimiter(
            max_requests=self.engine_config.rate_limit_requests,
            window_seconds=self.engine_config.rate_limit_window
        )

        logger.info(f"Config reloaded for: {self.brand_id}")

    def get_debug_info(self) -> Dict[str, Any]:
        """ë””ë²„ê·¸ ì •ë³´"""
        if not self.engine_config.debug_mode:
            return {'debug_mode': False}

        return {
            'debug_mode': True,
            'brand_id': self.brand_id,
            'state': self._state.value,
            'config': {
                'cache_enabled': self.engine_config.cache_enabled,
                'rate_limit_enabled': self.engine_config.rate_limit_enabled,
                'rate_limit_requests': self.engine_config.rate_limit_requests,
                'request_timeout': self.engine_config.request_timeout,
            },
            'router_info': self.router.get_debug_info() if hasattr(self.router, 'get_debug_info') else {},
            'metrics': self.metrics.get_summary(),
            'middlewares': [m.__class__.__name__ for m in self._middlewares],
        }

    # === Private Methods ===

    def _check_engine_state(self):
        """ì—”ì§„ ìƒíƒœ í™•ì¸"""
        if self._state == EngineState.STOPPED:
            raise EngineError(
                "ì—”ì§„ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                ErrorType.INTERNAL
            )

        if self._state == EngineState.SHUTTING_DOWN:
            raise EngineError(
                "ì—”ì§„ì´ ì¢…ë£Œ ì¤‘ì…ë‹ˆë‹¤.",
                ErrorType.INTERNAL
            )

    def _generate_request_id(self, question: str) -> str:
        """ìš”ì²­ ID ìƒì„±"""
        data = f"{self.brand_id}:{question}:{time.time()}"
        return hashlib.md5(data.encode()).hexdigest()[:12]

    def _build_response(
        self,
        context: QueryContext,
        metrics: RequestMetrics
    ) -> ChatResponse:
        """ChatResponse ê°ì²´ ìƒì„±"""
        # Retrieval ì»¨í…ìŠ¤íŠ¸ ë³€í™˜
        retrieval_contexts = []

        for result in context.retrieval_results:
            retrieval_contexts.append(
                RetrievalContext(
                    source=result.source,
                    data=result.data,
                    metadata=result.metadata
                )
            )

        # ë©”íƒ€ë°ì´í„°
        metadata = {
            'request_id': metrics.request_id,
            'response_time_ms': round(metrics.total_time * 1000, 2),
            'routing_time_ms': round(metrics.routing_time * 1000, 2),
            'retrieval_time_ms': round(metrics.retrieval_time * 1000, 2),
            'generation_time_ms': round(metrics.generation_time * 1000, 2),
            'retrieval_count': metrics.retrieval_count,
            **context.metadata
        }

        return ChatResponse(
            brand_id=self.brand_id,
            message=context.response or "",
            question_type=context.question_type.value if hasattr(context.question_type, 'value') else str(context.question_type),
            retrieval_contexts=retrieval_contexts,
            metadata=metadata
        )

    def _format_retrieval_context(self, context: QueryContext) -> str:
        """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        if not context.retrieval_results:
            return "No relevant information found."

        formatted_parts = []

        for result in context.retrieval_results:
            if not result.data:
                continue

            formatted_parts.append(f"\n[{result.source.upper()}]")

            for i, item in enumerate(result.data[:5], 1):
                formatted_parts.append(f"{i}. {json.dumps(item, ensure_ascii=False)}")

        return "\n".join(formatted_parts)

    def _get_cached_response(self, question: str) -> Optional[ChatResponse]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        try:
            cache_key = f"{self.brand_id}:question:{hashlib.md5(question.encode()).hexdigest()[:16]}"
            cached = self.cache.get(cache_key)

            if cached:
                return ChatResponse.model_validate_json(cached)

            return None

        except Exception as e:
            logger.debug(f"Cache get error: {e}")
            return None

    def _cache_response(self, question: str, response: ChatResponse):
        """ì‘ë‹µ ìºì‹±"""
        try:
            cache_key = f"{self.brand_id}:question:{hashlib.md5(question.encode()).hexdigest()[:16]}"
            self.cache.set(
                cache_key,
                response.model_dump_json(),
                ttl=self.engine_config.cache_ttl
            )
        except Exception as e:
            logger.debug(f"Cache set error: {e}")

    def _handle_error(
        self,
        error: EngineError,
        metrics: RequestMetrics,
        question: str
    ) -> ChatResponse:
        """ì—ëŸ¬ ì²˜ë¦¬"""
        metrics.error = error.message
        metrics.end_time = time.time()
        self.metrics.record_request(metrics)

        # ë¯¸ë“¤ì›¨ì–´ ì—ëŸ¬ ì²˜ë¦¬
        context = QueryContext(brand_id=self.brand_id, question=question)
        for middleware in self._middlewares:
            result = middleware.on_error(context, error)
            if result:
                return result

        # ì—ëŸ¬ ì¶”ì 
        self.analytics.track_event(
            'error',
            self.brand_id,
            {
                'question': question[:100],
                'error_type': error.error_type.value,
                'error_message': error.message,
                **error.details
            }
        )

        # ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€
        user_messages = {
            ErrorType.VALIDATION: error.message,
            ErrorType.RATE_LIMIT: "ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            ErrorType.TIMEOUT: "ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.",
            ErrorType.ROUTING: "ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            ErrorType.RETRIEVAL: "ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            ErrorType.GENERATION: "ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        }

        message = user_messages.get(
            error.error_type,
            "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )

        return ChatResponse(
            brand_id=self.brand_id,
            message=message,
            metadata={
                'error': True,
                'error_type': error.error_type.value,
                'request_id': metrics.request_id,
            }
        )

    # === Validation v2.0 Methods ===

    def _validate_and_retry(
        self,
        context: QueryContext,
        metrics: RequestMetrics,
        question: str
    ) -> tuple:
        """
        ì‘ë‹µ ê²€ì¦ ë° í•„ìš”ì‹œ ì¬ìƒì„±

        Args:
            context: í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
            metrics: ìš”ì²­ ë©”íŠ¸ë¦­
            question: ì›ë³¸ ì§ˆë¬¸

        Returns:
            (updated_context, updated_metrics)
        """
        min_grade_priority = self._grade_priority.get(
            self.engine_config.validation_min_grade.upper(), 2
        )
        max_retries = self.engine_config.validation_max_retries
        retry_count = 0

        while retry_count <= max_retries:
            # ê²€ì¦ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            validation_context = {
                'question': question,
                'brand_id': self.brand_id,
                'retrieval_results': {
                    r.source: r.data for r in context.retrieval_results
                } if context.retrieval_results else {},
            }

            # ê²€ì¦ ì‹¤í–‰
            validation_result: ValidationResult = self.validation_filter.validate(
                response=context.response or "",
                context=validation_context
            )

            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            metrics.validation_score = validation_result.score
            metrics.validation_grade = validation_result.grade.value
            metrics.validation_retries = retry_count

            # ë“±ê¸‰ í™•ì¸
            current_grade_priority = self._grade_priority.get(
                validation_result.grade.value, 1
            )

            # ë¡œê¹…
            logger.info(
                f"[Validation] Grade={validation_result.grade.value}, "
                f"Score={validation_result.score:.2f}, "
                f"Retry={retry_count}/{max_retries}"
            )

            # í†µê³¼ ì¡°ê±´ í™•ì¸
            if current_grade_priority >= min_grade_priority:
                # ê²€ì¦ í†µê³¼ - ë©”íƒ€ë°ì´í„° ì¶”ê°€
                context.add_metadata('validation', {
                    'grade': validation_result.grade.value,
                    'score': validation_result.score,
                    'status': validation_result.status.value,
                    'retries': retry_count,
                })

                # ê°œì„  ì œì•ˆ ì¶”ê°€ (ì˜µì…˜)
                if self.engine_config.validation_include_suggestions and validation_result.suggestions:
                    context.add_metadata('improvement_suggestions', validation_result.suggestions)

                # ê²½ê³ ê°€ ìˆìœ¼ë©´ ë¡œê·¸ì— ê¸°ë¡
                if validation_result.all_warnings:
                    logger.warning(
                        f"[Validation Warnings] {', '.join(validation_result.all_warnings[:3])}"
                    )

                return context, metrics

            # ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not self.engine_config.validation_auto_retry or retry_count >= max_retries:
                break

            # ì¬ìƒì„± ì‹œë„
            logger.warning(
                f"[Validation] Low grade ({validation_result.grade.value}), "
                f"attempting regeneration ({retry_count + 1}/{max_retries})"
            )

            # ì´ìŠˆë¥¼ í”„ë¡¬í”„íŠ¸ íŒíŠ¸ë¡œ ì¶”ê°€
            issues_hint = "; ".join(validation_result.all_issues[:3]) if validation_result.all_issues else ""
            context.add_metadata('regeneration_hint', issues_hint)

            # ì¬ìƒì„±
            try:
                context = self.pipeline._generate(context)
                retry_count += 1
            except Exception as e:
                logger.error(f"[Validation] Regeneration failed: {e}")
                break

        # ìµœì†Œ ë“±ê¸‰ ë¯¸ë‹¬ - ì‚¬ê³¼ ë©”ì‹œì§€ ì¶”ê°€
        if current_grade_priority < min_grade_priority:
            context = self._append_apology_message(context, validation_result)
            context.add_metadata('validation', {
                'grade': validation_result.grade.value,
                'score': validation_result.score,
                'status': 'below_threshold',
                'retries': retry_count,
                'issues': validation_result.all_issues[:5],
            })

            logger.warning(
                f"[Validation] Response below minimum grade after {retry_count} retries. "
                f"Final grade: {validation_result.grade.value}"
            )

        return context, metrics

    def _append_apology_message(
        self,
        context: QueryContext,
        validation_result: ValidationResult
    ) -> QueryContext:
        """ë‚®ì€ í’ˆì§ˆ ì‘ë‹µì— ì‚¬ê³¼ ë©”ì‹œì§€ ì¶”ê°€"""
        apology_messages = {
            'F': "\n\n---\nâš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤. ì¶©ë¶„íˆ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.",
            'D': "\n\n---\nğŸ’¡ ì°¸ê³ : ì´ ë‹µë³€ì€ ì œí•œëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.",
        }

        grade = validation_result.grade.value
        if grade in apology_messages:
            current_response = context.response or ""
            context.set_response(current_response + apology_messages[grade])

        return context

    def _is_grade_acceptable(self, grade: str) -> bool:
        """ë“±ê¸‰ì´ í—ˆìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
        min_priority = self._grade_priority.get(
            self.engine_config.validation_min_grade.upper(), 2
        )
        current_priority = self._grade_priority.get(grade.upper(), 1)
        return current_priority >= min_priority

    def _shutdown(self):
        """ì—”ì§„ ì¢…ë£Œ"""
        self._state = EngineState.SHUTTING_DOWN
        logger.info(f"Shutting down engine for: {self.brand_id}")

        # ì •ë¦¬ ì‘ì—…
        self.rate_limiter.reset()

        self._state = EngineState.STOPPED

    def __repr__(self) -> str:
        return (
            f"UniversalEngine(brand={self.brand_id}, "
            f"state={self._state.value}, "
            f"requests={self.metrics.total_requests})"
        )
